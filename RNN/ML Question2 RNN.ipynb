{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML Question2 Assignment6.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1o7sIrvMjNW6-xhTV0W-lHzvnmhizys0j","authorship_tag":"ABX9TyNKfIhUS0QLdcywP13fglnd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3k15ZJwAnDpH","executionInfo":{"status":"ok","timestamp":1607123430978,"user_tz":-330,"elapsed":1351,"user":{"displayName":"Shreya Goel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ8jg04dSe2ZK-kSy1T3zYJoyCvGXM9Sord18i=s64","userId":"09822123549759748139"}},"outputId":"e1168f62-117e-43b8-8766-8bdec1eba97b"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZKUVz63pKN0","executionInfo":{"status":"ok","timestamp":1607123435768,"user_tz":-330,"elapsed":6122,"user":{"displayName":"Shreya Goel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ8jg04dSe2ZK-kSy1T3zYJoyCvGXM9Sord18i=s64","userId":"09822123549759748139"}},"outputId":"4adbcf99-351f-4d4a-bd5b-9bb624021c00"},"source":["#Library to perform operations\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","#Library to perform train test split\n","from sklearn.model_selection import train_test_split\n","\n","#Library to perform PCA\n","from sklearn.decomposition import PCA\n","from sklearn import preprocessing \n","#Library to calculate accuracy\n","from sklearn.metrics import accuracy_score\n","\n","#Library to load data\n","import re\n","#Library to perform text preprocessing\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","import string\n","\n","#Library for RNN neural networks\n","from numpy import array\n","from numpy import asarray\n","from numpy import zeros\n","\n","#Keras Layers\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers.recurrent import SimpleRNN\n","from keras.layers import Bidirectional\n","from keras.layers import Dense\n","from keras.models import Sequential\n","from keras.layers import GlobalMaxPooling1D\n","from keras.layers.embeddings import Embedding\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nOQnJNsZX3TF"},"source":["### **A part**"]},{"cell_type":"markdown","metadata":{"id":"l6xgIrTzX7fA"},"source":["**Downloading and Loading the Dataset**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"WTn03bjlpPWw","executionInfo":{"status":"error","timestamp":1607123435815,"user_tz":-330,"elapsed":6146,"user":{"displayName":"Shreya Goel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ8jg04dSe2ZK-kSy1T3zYJoyCvGXM9Sord18i=s64","userId":"09822123549759748139"}},"outputId":"fa16fb94-d6b1-488c-9c58-b7b266134375"},"source":["#Loading the Dataset\n","data = []\n","with open('/content/ML_A6_Q2_data.txt') as fh:\n","    for line in fh:\n","      data.append(re.split('\\t|\\n',line))\n","data"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-0c8e36af0cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Loading the Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/ML_A6_Q2_data.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t|\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/ML_A6_Q2_data.txt'"]}]},{"cell_type":"code","metadata":{"id":"XSZVj9yXpdsK"},"source":["#Making a dataframe from the loaded data\n","data_2 = pd.DataFrame(data, columns = ['Sentence','Tag','d'])\n","data_2 = data_2.drop(['d'],axis = 1)\n","print(data_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jvrmsFM6p7mx"},"source":["print(data_2.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kLecCVzipjIu"},"source":["data_2.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9FiQr9KVVODE"},"source":["#Countplot of the 0 and 1 labels\n","import seaborn as sns\n","\n","sns.countplot(x='Tag', data=data_2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jeuPMI_ecZT"},"source":["### **B part**"]},{"cell_type":"markdown","metadata":{"id":"kAAxSlOXtduC"},"source":["**Preprocess the data**"]},{"cell_type":"code","metadata":{"id":"etLa4110Xx_o"},"source":["def text_preprocess(message):\n","    #Checking if the characters are in punctuation\n","    punc_remove = [char for char in message if char not in string.punctuation]\n","    #Forming the String again by joining the letters\n","    punc_remove = ''.join(punc_remove)\n","    #Making the string to lower case after removing punctuations\n","    lower_str = \"\"\n","    for word in punc_remove.split():\n","      lower_str += word.lower() + \" \"\n","\n","    #Removing the Stopwords\n","    for word in lower_str.split():\n","      return [word for word in lower_str.split() if word not in stopwords.words('english')]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTa4oUzqMQEB"},"source":["#Preprocessing the Data\n","for i in range(data_2.shape[0]):\n","  data_2['Sentence'][i] = text_preprocess(data_2['Sentence'][i])\n","print(\"Text Preprocessed Data\")\n","print(data_2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcJ066u9thbz"},"source":["**Splitting into train and test data**"]},{"cell_type":"code","metadata":{"id":"JlVBC1O2Lu8G"},"source":["#Dividing the Data into X and Y values\n","data_x = data_2.iloc[:,:-1]\n","data_y = data_2.iloc[:,-1]\n","print(data_x)\n","print(data_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y7mBzLSeLoZ2"},"source":["#Dividing the data into train and test with 70:30\n","X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.30, random_state=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2GMiCh7RA1q"},"source":["X_train = X_train.reset_index(drop=True)\n","y_train = y_train.reset_index(drop=True)\n","X_test = X_test.reset_index(drop = True)\n","y_test = y_test.reset_index(drop = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1j7diIDXPywA"},"source":["#Printing the X_train and X_test\n","print(\"X_train\")\n","print(X_train)\n","print(\"\\nX_test\")\n","print(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lHe-nNYTtsFY"},"source":["**Getting the training and testing words vocabulary**"]},{"cell_type":"code","metadata":{"id":"tvSYGoPb0rL8"},"source":["#Getting all the training words\n","all_words = [word for sen in X_train['Sentence'] for word in sen]\n","#Printing the Sentence length\n","all_word_length = [len(sen) for sen in X_train['Sentence']]\n","#Storing the maximum length\n","max_length = max(all_word_length)\n","#Storing the training vocabulary\n","train_vocab = set(all_words) \n","train_vocab = sorted(list(train_vocab))\n","#Printing the training vocabulary\n","print(train_vocab)\n","print(max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8P-bb3ZiRxJP"},"source":["#Getting all the testing words\n","all_words_test = [word for sen in X_test['Sentence'] for word in sen]\n","#Printing the Sentence length\n","all_testword_length = [len(sen) for sen in X_test['Sentence']]\n","#Storing the maximum length of the test data\n","test_max_length = max(all_testword_length)\n","#Storing the testing vocabulary\n","test_vocab = set(all_words_test) \n","test_vocab = sorted(list(test_vocab))\n","#Printing the testing vocabulary\n","print(test_vocab)\n","print(test_max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ENEH5hutfpyK"},"source":["**Pre-Trained GLoVe**"]},{"cell_type":"code","metadata":{"id":"Da2V2iYSSFdd"},"source":["#Storing the embeddings in a dictionary\n","embeddings_dict = dict()\n","#Open the glove file\n","glove_file = open('/content/drive/MyDrive/glove.6B.50d.txt', encoding=\"utf8\")\n","#Iterating over each line\n","for line in glove_file:\n","    #Splitting the line\n","    data = line.split()\n","    #Extracting the word from the list made of each line\n","    word = data[0]\n","    #Storing the dimensions of the word in dictionary\n","    word_dimension = asarray(data[1:], dtype='float32')\n","    embeddings_dict[word] = word_dimension\n","\n","glove_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DndGS4P-lAqd"},"source":["#Printing the embeddings for word very\n","print(embeddings_dict.get('very'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Symuy3tty4d"},"source":["**Making token and converting the sentence to respective sequences**"]},{"cell_type":"code","metadata":{"id":"2FBk8taXQGj1"},"source":["#Making the Tokens by using the tokenizer class\n","tokenizer = Tokenizer(num_words=len(train_vocab),lower=True, char_level=False)\n","#Fir the tokens according to the training data sentences\n","tokenizer.fit_on_texts(X_train['Sentence'])\n","#Converting the training words into numeric data sequences\n","X_train = tokenizer.texts_to_sequences(X_train['Sentence'].tolist())\n","#Converting the testing words into numeric data sequences\n","X_test = tokenizer.texts_to_sequences(X_test['Sentence'].tolist())\n","print(\"Training Sequences\")\n","print(X_train)\n","print(\"Testing Sequences\")\n","print(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXSziCPLt7n7"},"source":["**Padding the train and test data**"]},{"cell_type":"code","metadata":{"id":"R-3BdDi4RbFM"},"source":["#Getting the vocabulary length and adding 0 because of reserved index\n","vocabulary_length = len(tokenizer.word_index) + 1\n","\n","#Printing the tokenizer word and index dictionary\n","print(tokenizer.word_index.items())\n","\n","#Keeping the max_length as 50\n","max_length = 50\n","\n","#Padding the sequences of the train data\n","X_train = pad_sequences(X_train, padding='post', maxlen=max_length)\n","#Padding the sequences of the test data\n","X_test = pad_sequences(X_test, padding='post', maxlen=max_length)\n","\n","print(\"\\nTraining Data after Padding\")\n","print(X_train)\n","print(\"\\nTesting Data after Padding\")\n","print(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fpPU1yKTt_uh"},"source":["**Making the Embedding matrix**"]},{"cell_type":"code","metadata":{"id":"Q5kFlsSFjPXD"},"source":["#Making an embedding matrix of vocab size*50 dimensions\n","embedding_matrix = zeros((vocabulary_length, 50))\n","\n","#Iterating over every word in the vocabulary from training data\n","for word, index in tokenizer.word_index.items():\n","    #Getting the word dimension for every word of the vocabulary from the glove data\n","    word_dimension = embeddings_dict.get(word)\n","    #If word dimension is present\n","    if word_dimension is not None:\n","        #Store the word dimension into the matrix\n","        embedding_matrix[index] = word_dimension\n","\n","print(\"The shape of the Embedding Matrix \", embedding_matrix.shape)\n","print(\"\\nEmbedding Matrix\")\n","print(embedding_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NuIIvtnkuHz1"},"source":["### **C part**"]},{"cell_type":"markdown","metadata":{"id":"_bpRyu7WuNYw"},"source":["**Making the network architecture**"]},{"cell_type":"code","metadata":{"id":"8nW925oWVq_h"},"source":["#Loading the Sequential model\n","model = Sequential()\n","#Adding the Embedding Layer\n","model.add(Embedding(vocabulary_length, 50, weights=[embedding_matrix], input_length=max_length , trainable=False))\n","#Adding the Bidirectional Simple RNN layer\n","model.add(Bidirectional(SimpleRNN(50,return_sequences = True)))\n","#Adding the GlobalMaxPooling1D Layer\n","model.add(GlobalMaxPooling1D())\n","#Adding the Dense layer\n","model.add(Dense(1, activation='sigmoid'))\n","#Compiling the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iLY-PCLnurg8"},"source":["print(\"The Summary of the Model\")\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxhhv_Y26FcX"},"source":["#Converting the training and testing labels into array\n","train_list = [int(i) for i in y_train] \n","test_list = [int(i) for i in y_test] \n","y_train = np.array(train_list)\n","y_test = np.array(test_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EH7Lfs1FuwKw"},"source":["#Fitting the models over training data and evaluating scores\n","trained_model = model.fit(X_train, y_train, batch_size=128, epochs=50, verbose=1,validation_split=0.2)\n","score_train = model.evaluate(X_train, y_train, verbose=1)\n","score_test = model.evaluate(X_test, y_test, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Y9yKx-pxUwL"},"source":["### **D part**"]},{"cell_type":"code","metadata":{"id":"P7ze08wMu0mc"},"source":["print(\"Training Loss:\", score_train[0])\n","print(\"Training Accuracy:\", score_train[1])\n","print(\"Validation Loss:\", score_test[0])\n","print(\"Validation Accuracy:\", score_test[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t1CT0DrqzRdT"},"source":["**Accuracy Plot**"]},{"cell_type":"code","metadata":{"id":"Kh-db4a27n2L"},"source":["#Plot the accuracy over training and testing data with epoch\n","plt.plot(trained_model.history['acc'])\n","plt.plot(trained_model.history['val_acc'])\n","\n","plt.title('Accuracy of Model')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epochs')\n","plt.legend(['train','test'], loc='upper left')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8HIH77jBzUzP"},"source":["**Loss Plot**"]},{"cell_type":"code","metadata":{"id":"TQ2lZR4lzKr9"},"source":["#Plot the loss over training and testing data with epoch\n","plt.plot(trained_model.history['loss'])\n","plt.plot(trained_model.history['val_loss'])\n","\n","plt.title('Loss of Model')\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","plt.legend(['train','test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlJv0Ci38dqu"},"source":["predictions = model.predict(X_test, batch_size=1024, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-MxXWVs8qZ6"},"source":["predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7V_LCyy18jty"},"source":["prediction_labels=[]\n","for p in predictions:\n","    prediction_labels.append(list(np.around(p)))\n","import itertools\n","flat=itertools.chain.from_iterable(prediction_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ht03Gox0HsWQ"},"source":["flat = list(flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fDohNRTDURYW"},"source":["flat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQl3to1_HsKW"},"source":["print(accuracy_score(flat,y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vud4QcKhuXXY"},"source":["# model = Sequential()\n","# embedding_layer = Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n","# model.add(embedding_layer)\n","# model.add(LSTM(128))\n","# model.add(SimpleRNN(units = 100, activation='relu',use_bias=True))\n","# #model.add(Dense(1, activation='sigmoid'))\n","# model.add(Dense(units=1000, input_dim = 2000, activation='sigmoid'))\n","# model.add(Dense(units=500, input_dim=1000, activation='relu'))\n","# model.add(Dense(units=2, input_dim=500,activation='softmax'))\n","# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"],"execution_count":null,"outputs":[]}]}